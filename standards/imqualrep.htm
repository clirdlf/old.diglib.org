<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">

<html>



<head>



<meta http-equiv="Content-Type"

content="text/html; charset=iso-8859-1">




<title>Assessing the Quality of Digital Images (30 March 2001)</title>

</head>



<body background="../img/DLFsidebar.gif" bgcolor="#FFFFFF"

LINK="#006600" alink="#660000"vlink="#CC6600" text="#000000">



<!-- Begin Table -->

<table border="0" cellpadding="0" cellspacing="0" width="100%">



<!-- DLF Graphic -->



<tr>

<td valign="bottom" align="left" colspan=3>

<IMG SRC="../img/spacer.gif" WIDTH=12 HEIGHT=1 BORDER=0 ALIGN="bottom" ALT="">

<img src="../img/dlflogosquare.gif" alt="DLF logo" align="bottom" width="56" height="50" hspace="0" vspace="15">

<IMG SRC="../img/spacer.gif" WIDTH=20 HEIGHT=1 BORDER=0 ALIGN="bottom" ALT="">

<img src="../img/dlflogotext.gif" alt="DLF logo" align="bottom" width="282" height="41" hspace="1" vspace="15">

</td>

</tr>

<tr>

<td valign="top" align="left" colspan=3>

<IMG SRC="../img/spacer.gif" WIDTH=100 HEIGHT=1 BORDER=0 ALIGN="bottom" ALT="">
</td>


</tr>



<!-- Navigation column -->    <tr>

 	<TD valign="top" ALIGN="left" WIDTH=180>

 	<IMG SRC="../img/spacer.gif" WIDTH=180 HEIGHT=1 BORDER=0 ALIGN="LEFT" ALT="">

	<BR CLEAR="ALL">

<a href="../dlfhomepage.htm"><IMG SRC="../img/rdlfhome.gif" WIDTH="155" HEIGHT="16" ALT="DLF Home" BORDER=0></a>


 	    <P><a href="../about.htm"><IMG SRC="../img/rabout.gif" WIDTH="155" HEIGHT="16" ALT="About" BORDER=0></a>



	    <P><a href="../architectures.htm"><IMG SRC="../img/rarch.gif" WIDTH="155" HEIGHT="16" ALT="Architectures, systems and tools" BORDER=0></a>



     	<P><a href="../preserve.htm"><IMG  SRC="../img/rpresv.gif" WIDTH="155" HEIGHT="16" ALT="Digital preservation" BORDER=0></a>



     	<P><a href="../collections.htm"><IMG SRC="../img/rcoll.gif" WIDTH="155" HEIGHT="16" ALT="Digital collections" BORDER=0></a>


     	<P><a href="../standards.htm"><IMG SRC="../img/rsandp.gif" WIDTH="155" HEIGHT="16" ALT="Standards and practices" BORDER=0></a>

     	<P><a href="../use.htm"><IMG SRC="../img/ruandu.gif" WIDTH="155" HEIGHT="16" ALT="Use and users" BORDER=0></a>



     	<P><a href="../roles.htm"><IMG  SRC="../img/rrandr.gif" WIDTH="155" HEIGHT="16" ALT="Roles and responsibilities" BORDER=0></a>

     	<P><a href="../forums.htm"><IMG  SRC="../img/rforum.gif" WIDTH="155" HEIGHT="16" ALT="DLF Forum" BORDER=0></a>


     	<P><a href="../publications.htm"><IMG  SRC="../img/rpandr.gif" WIDTH="155" HEIGHT="16" ALT="Publications and resources" BORDER=0></a>

        </td>



<!-- Gutter column -->

	<td valign="top">

		<img src="../img/spacer.gif" alt="" align="top" width="25" height="1">

		</td>



<!-- Text column -->

    <td valign="top" width="75%">

<!-- Begin text here -->
<H2>Report of Imaging Practitioners Meeting on 30 March 2001 to Consider How the Quality of Digital Imaging Systems and Digital Images May be Fairly Evaluated</H2>

S. Chapman<Br>
submitted to DLF May 23, 2001

<P><U>Present </U>(20):<BR>
Sally Bjork (UM), Stephen Chapman (HUL), Bill Comstock (HCL), Franziska Frey (IPI), Hans Hansen (Octavo), Dan Johnston (UCB), Erik Landsberg (MoMA), Lee Mandell (HUL), David Mathews (MFA), Phil Michel (LC), Stephanie Mitchell (HCL), Ron Murray (LC), Alan Newman (AIC), Steve Puglia (NARA), David Remington (HCL), David Semperger (BosPhoto), Peter Siegel (Harvard), Don Williams (Kodak), John Woolf (MFA), Mingtao Zhao (HCL)

<P><U>Other invited guests</U>:<BR>
present (a.m.):       Jan Merrill-Oldham (HUL), Mark Roosa (LC)<BR>
unable to attend:   J. Meyer (Digital Attributes), D. Zabriskie (Luna), K. Kallsen (HUAM)

<P>see <a href="#app1">Appendix I</a>, "List of Participants," for more information

<H3>Overview</H3>

<P>By 1998, several consortia recognized that significant investments were being made to digitize pictorial collections in libraries, archives and museums.  This activity was and continues to be highly decentralized.  In the absence of guidelines or best practices for image digitization, one concern was whether these collections would be interoperable: were images being made in formats that could be easily distributed? And to a baseline level of quality that would meet users' requirements?  If one looks back even earlier to the experiences of MESL and other projects to create and exchange visual images, it seems that the focus shifted at one point from making digital images to making good digital images.   

<P>With these issues of imaging investments in mind, the Digital Library Federation and the Research Libraries Group sponsored a series of imaging guides that were published in July 2000.  The guides were designed to help make project managers and technicians aware of the decisions-e.g., selecting and setting up equipment-that would have the greatest impact upon image quality.  Parallel to this effort, a NINCH working group had been advancing its efforts to codify best practices for digitizing cultural heritage materials and was eager to receive feedback from the practitioners with the greatest amount of field experience.

<P>The purpose of this DLF-sponsored meeting was to establish a forum for expert practitioners to exchange ideas about what is "good" and, if possible, to prioritize where tools, applications, and training would be of greatest benefit to meet our institutions' obligations to make digital reproductions of consistent quality and persistent utility.

<H3>High-level statement of the problem</H3>

<P>Don Williams, an image scientist from Eastman Kodak who facilitated the meeting, spoke both for imaging practitioners and for imaging scientists and other members of standards committees when he noted, "There appears to be somewhat of a consensus that there is not any reasonable way right now to look at all imaging performance measures without ambiguity." [<a href="#fn1">1</a>]   

<P>Subjective assessments are notoriously flawed, due not only to differences among human observers, but to limitations of devices that render images (monitors, printers), as well as the differences in ambient lighting in two or more viewing environments.

<P>Objective methodologies are also imperfect.  Although quantifiable metrics are well defined in the scientific community-and currently receiving greater endorsement by standards organizations-it is currently very difficult to use tools to measure imaging performance in real-world production environments. Making an analogy to changes in auto manufacturing, Don pointed out that photographers no longer know "what's under the hood" in their imaging systems.  It is difficult enough to find tools to interrogate imaging performance-in key areas such as noise, density and dynamic range-but even more problematic to subtract the errors that may have been inadvertently introduced by the process itself.  Does a noise measurement, for example, refer to the imaging system or the noise generated by a target that was digitized? 

<P>Intentional or unintentional use of imprecise terminology also creates ambiguity.  For example, industry's marketing literature and our community's funding guidelines routinely associate image quality with unreliable metrics-such as resolution and bit depth.  These performance characteristics refer to input settings and become ambiguous if used to describe output quality.  The same source could be digitized by two systems that produce the same nominal results (e.g, 3,000 pixel, 24-bit RGB images) yet the quality of the images may differ significantly.

<P>Finally, as Ron Murray would point out at the meeting, there is a potential to confuse measurements with judgments (how people see images and whether they will be satisfied).  The challenge is to integrate these two perspectives of quality into methods for quality control.

<P>Several challenges brought imaging experts to this forum: to improve upon the conventions used to describe and distinguish the performance of various imaging systems; to institutionalize methods to monitor performance of equipment; and to simplify the task of evaluating images, particularly those produced outside of known workflows.

<H3>I.  Summary of current practices to monitor quality of systems and images</h3>

The meeting began with a <I>tour de table</I>, with each photographer describing quality requirements and production practices in his or her studio.  To initiate discussion on image quality, rather than production per se, Don Williams encouraged each person to answer three questions, listed below.  Since viewpoints were remarkably similar among the commercial (2), museum (4), and library/ archives (5) communities that were represented, responses are categorized by question, rather than by type of studio.  

<P><I>Note: other questions were distributed to participants in advance.  Their responses,  summarized in <a href="#app2">Appendix II</a>, provide a more complete picture of institutional practices.</I>


<H4>1)  How did you choose your equipment and with what priorities? </h4>
<UL>
<P><LI><B>priorities</b>
<UL>
<LI>project-driven demands
<UL>
<LI>budget, expectations for productivity, and end use (of images) are major factors: "...evaluate what file will be used for and what they're willing to spend... produce samples and show them with price tag attached..."
<LI>materials handling requirements can limit choice (e.g., UC Berkeley needed a flatbed scanner with variable focus in order to scan papyri in 1 x 14 glass mounts)
</UL>
<LI>product support
<UL>
<LI>availability of local service is critical; responsiveness very important, particularly when modifications are needed to keep studio up and running
</ul>
<LI>technical performance
<UL>
<LI>good dynamic range
<LI>quality of the signal (noise)[<a href="#fn2">2</a>]
<LI>sharpness: images must be consistently sharp corner to corner
<LI>support 16-bit output
<LI>ease of use
<LI>fit/integration into existing workflows
<LI>device-independence (openness)
</UL></UL>
<P><LI><B>techniques used to choose equipment</b>
<UL>
<LI>viable methods...
<UL>
<LI>advice/feedback from colleagues-"perhaps the most valuable short cut"
<LI>research (read Seybold reports, ColorSync list, etc.)
<LI>demonstration and testing in one's own studio, with qualifications: although it is often possible to bring in equipment, it is very difficult to accomplish any meaningful testing in a production environment; heads nodded when one photographer said he "would love to have a separate lab just for testing"
<LI>evaluation of technical targets  
<LI>soft-proofing and/or evaluation of prints (of "known outputs")
</UL>

<LI>methods that don't work...
<UL>
<LI>manufacturers' specifications are universally distrusted
<LI>evaluating equipment at trade shows-technical reps not present; difficult to scan materials at settings you want to use
</UL></UL></UL>
<H4>2)  How do you monitor system's imaging performance (e.g. tools, frequency, methods)?</H4>

<UL>
<P><LI><B>visual assessment </b>
<UL>
<LI>evaluation of images on screen
<UL>
<LI> "if something is going wrong we can see it in the 100% view of the image" (AIC)
<LI>in high-production workflow, only practical choice is to review small sample (sometimes as few as 1 in 200) (LC)
<LI>trouble areas to watch for are dust and reproducing corners as sharp as the center (getting the original aligned flat is challenging)
<BR>...requires monitor calibration (generally monthly) 
<LI><I>note</I>:  heads nodded with the assertion that the monitor is the weak link in this type of quality assessment
<LI>the X-Rite DT92Q colorimeter and OptiCal software "have produced good results" on a variety of monitors in a variety of settings (AIC, MFA, HCL, NARA), although opinions vary about the quality of the monitors themselves. HCL is satisfied with Barcos; the MFA with Mitsubishi DIAMONDTRONs;   AIC has established aim point densities for highlight and shadow at 6500ºK and eliminated monitors that failed to produce a bright enough white point (according to the photographer's eye) 
</UL>
<LI>calibration of ambient environment
<UL>
<LI>varying degrees of familiarity and interest in implementing the ISO 3664 standard, <I>Viewing conditions - Graphic technology and photography</I>
</UL></UL>
<P><LI><B>target-based assessment</b> (per setup, daily, or weekly)
<UL>
<LI>detail reproduction
<UL>
<LI>variety of targets/variety of uses: RIT line pairs (RT-1-71), SinePatterns Sinusoidal Test Pattern MTF target (used by LC principally to qualify vendor, but also routinely scanned in production), slanted-edge MTF target used to monitor equipment and select new components (e.g., lenses)
</UL>
<LI>color reproduction
<UL>
<LI>variety of targets: Macbeth ColorChecker and ColorChecker DC, Kodak Q-60 
</UL></UL>
<P><LI><B>reliance upon "native" calibration and standard service</b>
<UL>
<LI>daily auto-calibration and self-diagnostics, preventative maintenance calls by professional technicians (i.e., no secondary assessment)
</UL></UL></UL>

<H4>3)   What techniques have you instituted to achieve consistency in your images? 
As a corollary, What standards have you integrated into your imaging workflows?</h4>

<P>It is notable that no one mentioned specific standards.  A range of practices was described. All studios include grayscales and color bars in frame with reflective source materials.  But not all use them in the same ways.  This raises important questions about image exchange and image evaluation.  
<UL>
<P><LI><B>use of grayscale for tone reproduction </b>
<UL>
<LI>all studios include Kodak grayscales and color bars in-frame when digitizing  reflective media with consistent lighting ...the reason? as noted by Erik Landsberg, "targets and conditions provide a thread; know they're doing things consistently" 
<UL>
<LI>alternative method for film scanning and/or variable lighting setups not discussed</UL>
<LI>scan to aim points-varying numbers/methods: 
<UL>
<LI>aim points to grayscale based on gamma 1.8 curve (UCB and HCL)
<LI>HCL measure patches (of grayscale and Q-60) with colorimeter, then create curves to control contrast and gray balance during photography
<LI> "people know their numbers for highlight, midpoint, shadow": method is to hit aim points and keep RGB within two points throughout the scale (MoMA)
<LI>use aim points for shadow and highlights (AIC)
<LI>set middle gray to grayscale (UM)
</UL>
<LI>MFA has developed system to do lots of soft proofing to original based upon tonal scale and the characteristics of original 
<LI>MFA generates a "digital grayscale" to lock in RGB values calibrated to their system:  the monitor is "the only set point in whole system"-image on screen matched to original artwork illuminated in GTI viewing booth
<LI>MFA also creates "dead-neutral" 11-step grayscale in Photoshop...
</UL>

<P><LI><B>use of color bars for color reproduction</b>
<UL>
<LI>pin hopes on color management by using consistent lighting techniques; keep Q-60 scans and Macbeth color checker scans and associate them with corresponding files; hope to make input profiles from these (UCB)
<LI>metamerism takes place...numbers on grayscales and color bars not valid when shooting acrylics; color matching in the print comes down to perceptual issues... 
</UL>
<P><LI><B>subjective assessment</b>
<UL>
<LI>production-line monitoring; always scan in two-person team...two pairs of eyes (very much by observation); not using targets; using considered judgment; use best equipment we can (MoMA, UM); "[this technique] can be hit or miss, because production is such high priority"
</UL>
<P><LI><b>other</b>
<UL>
<LI>periodic cleaning and maintenance (static controls, humidity, dust)

<P>At one point in the discussion, Don Williams asked what kinds of problems studios might have in exchanging images?  The replies:

<LI>we would want to know something about how image was produced; would want to know which profile, target, curves were used-image metadata needed
<LI>one approach to manage image exchange: create a guide print (e.g., Pictograph, high-end Epson); match image on monitor to the print, then send out an RGB file-the huge assumption behind this technique is that the second studio will look at the print under same light source; proofing lights vary (e.g., GTI and Macbeth tubes produce noticeable differences in yellows and greens) 
<LI>high-end printers are the ones most resistant to digital photography...threatening their scanning and their separation...one museum studio described the challenges in developing workflows to deal with "a hostile environment" in sending out images for printing
</UL></UL></UL>

<H3>II.   Metrics, standards, and tools-presentation and demo by Don Williams </H3>

<p>Don Williams presented an overview of imaging criteria and some of the tools currently available to measure these.  He emphasized that the main objective of image performance metrics is to maintain a genealogical thread back to source document.  In the scientific community, it is well established that performance metrics go into image quality metrics.  He represented that there is a "lot of consensus [in this community] on how to measure imaging performance" and emphasized that these metrics do not need to dictate how you manage or produce your images, only on how to measure system performance and image quality. 

<P>Don clarified that his area of expertise is image microstructure.  He is most familiar with the tools-some already sanctioned by ISO-to measure resolution, grayscale tone, and image noise.  In part by working with the library and museum communities, Don has evolved from being a researcher to an applied researcher to a clinician.  What he has tried to impress upon standards committees-as Franziska Frey later echoed in her comments-are the workflow issues and concerns among practitioners.  

<P>Don argues that digital photography has not fundamentally changed imaging science-there's nothing new-but in digital imaging the photographer is responsible for knowing what's going on.  The idea is to get metrics out in the open so photographers can evaluate what they see is fit.

<UL>
<P><LI><B>key performance metrics</b>

<P><I>signal</I>

<UL>
<LI>tone capture/reproduction, spatial resolution (see below)
</ul>

<P><I>noise</I>
<UL>
<LI>random unstructured texture - 1D and 2D, artifacts, measure standard deviation in uniform textures and plot it
<LI>what's being evaluated...very hard to separate image processor from scanner....
<LI>can evaluate microstructure and compression behaviors with respect to resolution with digitally-generated images....
</UL>

<P><I>tone capture/reproduction</I>

<ul>
<LI>bits per sample is a pretty shallow spec: if there are non-uniformities in system corners get dark near the center...several bits used just for that (big problem with early Photo CD scanners)...use of bits for density detection the key
</UL>

<P><I>spatial resolution</I>

<UL>
<LI>sampling frequency (ppi, dpi) is a shallow spec-doesn't tell you much in terms of really knowing the amount of detail an imaging system can capture
<LI>bar targets-only method suitable to evaluate binary scanners, but with grayscale and color the cons are that these targets are contrast dependent and provide little diagnostic insight
<LI>MTF finding its way into standards development: use objective contrast criteria...plot ratio of contrasts...tells you extent to which contrasts degrade; example shows three outputs with same limiting resolution, but difference are important: one suggests flare, another well behaved, the third indicates sharpening (the hump at beg of curve)
<LI>emphasized that the MTF evaluation IS a predictor for image quality...one can cast just about any kind of an image process into an MTF...can take individual ones, cascade them and come up with a system MTF
<LI>...next step:  take performance metrics and come up with some kind of quality standard (take eye MTF and cascade with characteristics of imaging system...what's below curve is good)
</UL>

<P><I>Partifacts</I>
<UL>
<li>not too many tools to measure them
</UL>

<P><I>color registration</I>
<UL>
<li>work seems to be cyclical (not a problem in the late 1980s, then bad again, now lessening...); another strange artifact occurring in flatbed scanners is repetitive 2-D structure (wave going across the image...by interpolating down to 600, from say 610, not a problem with sharpening...but introduce sharpening and it screams at you)
</ul>

<P><LI><B>demo: Applied Image Slanted edge target and "Auto SFR Slant" software</b>

<UL>
<LI>virtues of system: inexpensive (all you need is an edge), designed to be used as "field tool" for quantitative analysis of noise, spatial resolution and color misregistration
<li>new target is ISO 16067 compliant; now only for reflectance; may be used for both camera and scanner; redundancy built in-spatial resolution estimated twice in both the horizontal and vertical dimensions
<li>software can plot noise at various density level
<li>MTF can be plotted for each color channel
<li><I>free offer from Don to scan slanted-edge targets </I> (see "Next Steps" at end of report)
</UL>

<P>
<i>Q&A revealed limitations to current tool</I>

<UL>
<li>cannot yet extract noise generated from the target-goal is to reliably extract noise due to scanner rather than noise to target
<li>some expressed need for real-time feedback (e.g., to assist in measuring focus)
<li>strong desire to measure "real dynamic range" with a target, since this cannot be seen on a monitor-would be very useful to assist in the set up phase of a shot/project
<UL>
<li>reply: will be discussing matter in 3 weeks.  Issue greater for reflective than transmissive. Proposal to measure dynamic range for cameras is embedded in OECF Standard ...scanner group to adopt and extend.
</UL>
<li>since there are competing targets/software being distributed from PIMA IT10, it is important for standards organizations either to adopt one set of tools or to overcome the current situation in which different image analysis packages may not give you the same numbers
</UL></UL>
<H3>III.  Metrics, standards, and tools-putting them into practice</H3>

<P>There were several exchanges about the applications for using targets, as well as the terms and conditions that would need to be met for managers to institute their use. Bill Comstock noted that "all of us are using targets...they're not useless [but] only useful under certain conditions and with certain types of source material."  The challenge is to articulate when they're useful and when they fail.  An observation was made that the worst thing they could do is use these things in an inappropriate way-worse to do that then not use them at all.  

<p>Thus, everything that follows for the remainder of this section should be classed as <I>Research Issues</I>.

<p>Observations were made that the two main applications are to monitor imaging system hardware, and/or to establish preferred processing actions (e.g., better compression; optimized amount of sharpening).  

<P>With respect to the technique of including targets in-frame with each item being digitized, an alternative was suggested: under set conditions, in the interests of saving time, could one scan the target separately and associate the target with the corresponding group of images in a database?  This idea seemed to be received equally with potential and with skepticism, with the skeptics asserting that the targets are needed to determine the response of the system under different conditions-specifically, the different conditions (reflectivity, etc.) driven by the source material itself.
<UL>
<P><LI><B>benefits to target-based assessment of quality</b>
<UL>
<LI>targets and associated imaging performance metrics can provide a "genealogical thread" back to the source
<li>can establish that imaging was done consistently
<li>targets function as common element to tell how far you've changed things
<li>targets can serve same purpose as control strips did in the "old days" of film processing: provided that one has an appropriate level of expertise/background of experience, control files made it readily apparent where something looks different than it did "last week": one can isolate how much work has been affected...
<li>agreement that image performance metrics are well understood by the scientific community 
<li>potential for improved troubleshooting and problem resolution: with targets and metrics, can better articulate problems with system components in discussion with vendors and manufacturers-avoid going round in circles with different eyes
<UL>
<li>evidence (HCL) already gathered that disproves the adage, "when something goes wrong you can see it"
</UL></UL>

<P><LI><b>dependencies</B>
<UL>
<LI>design and manufacture of targets (biggest concern):
<UL>
<li>practitioners insist targets must be of material similar to the source materials: spectral characteristics, contrast... comparing spectral responses of scanners to those of sources; will we ever have a target to assess a broad range of materials?
<li>some practitioners also want targets to be of similar size to source material, but DW "hesitates to recommend use of large target...problem with high contrast"
<li>need to be manufactured of material so "one has room for error"
<li>inconsistencies in manufacturing: "there are six different Macbeth targets...not even remotely alike; they age differently"
</UL>
<li>instrument calibration
<UL>
<li>e.g., must be certain that densitometers are properly calibrated if we were to rely upon these numbers to reconstruct images
<li>monitor densitometers must also be properly calibrated for soft proofing: one photographer asked, "How consistent is my X-Rite?  Does it really measure consistently?"  We're taking on faith that calibrator as delivered from X-Rite is working properly-should we have another instrument as a redundant check?
<li>aging of lamps, particularly LEDs in new scanners, a concern: how trustworthy is auto-calibration?  How do we assess consistency of lighting over the long term for such devices?
</UL>
<li>education and training
<UL>
<li>practitioners must use correct targets (e.g., group agreed that color targets that come with the grayscale are useless)
<li>we may need an intensive targeting phase to figure out what best imaging process is...versus telling us about systems...they are different objectives
</UL></UL>
<P><LI><B>present obstacles to implementation</b> (even if dependencies were adequately managed) 
<UL>
<li>two main perceptions:
<UL>
<li>tools not user friendly; ease of use is critical:  photographers do not want to "fuss idiosyncracies on a day-to-day basis"  ... "I'm still not convinced that this would be useful in day-to-day...if I have to introduce another step...[my boss] will say, 'Forget it'..."
<li>task more complex and time-consuming than soft proofing 
</UL></UL></UL>


<H3>IV.  Other Areas for Research and Development</H3>

<P>Although these topics received comparably less attention than performance metrics and targets, each represents a key challenge that demands further exchange of information and possibly collaborative investigation.  Selected commentary is recorded below.
<UL>
<P><LI><B>Documentation of practice/image metadata</b>

<P><LI><b>Conservation guidelines </b>
<UL>
<li>handling of bound materials
<UL>
<li>specifications about cradles and openability of books of particular interest to Octavo, but also to other participants
<li>depth of field a challenge
</UL>
<li>tolerable levels of light exposure for various materials 
</UL>
<P><LI><B>Color management</B>
<BR>"...standards chair says that when people talk about color there's never any agreement"

<P><I>Use of profiles</I>
<UL>
<LI>Macbeth charts fail because they're mapping all bit to large gamut (which is terrible if you are doing photography of near neutrals). Generic profiles are "virtually useless," but then observed that in some cases we're talking about failures in imaging, not color management.  To produce consistent quality, imaging needs to be right: black needs to be black, white needs to be white, and neutral needs to be neutral.  If we segregate out the target issues from the color management concerns, we can make progress.
<LI>we're reluctant to embed profiles, concerned about what happens down the line...question of doing something now that gives us 85% accuracy or forego ICC and "stick with numbers"
</UL>
<P><I>Color managed workflows</I>
<UL>
<LI>"pin hopes on color management by using consistent lighting techniques" 
<li>nothing inherently wrong with color management, but clearly problems with spectral responses and matching them to objects and sensors...color management good in a closed-loop system...there is issue with spectral sensitivity of sensors...by its very nature color management is an average and therefore a shortcoming
<li>only good way to manage color is to know spectral response..."second-generation" color management systems now being developed are spectral, not averaged: will allow one to build absolute profiles
</UL>
<P><I>Metamerism</I>
<UL>
<LI>differences between reflective spectra of commercial targets and many of the sources being digitized from libraries and museums 
<LI>differences in ambient lighting cause problems in color matching, particularly in printing
<UL>
<LI>create guide print (Fuji Pictograph, high-end Epson) that is matched to image on studio monitors; send RGB file to printer: "huge assumption" is that the print will be viewed under same light source used in studio; alternative: create color patches digitally matched to his system...his patches lock them into his monitor...correct reproduction on screen to match original...also create "dead-neutral" 11 step grayscale in Photoshop
<LI>found that proofing lights vary...went to GTI tubes instead of Macbeth tubes (noticeable differences in yellows/greens)
</UL>
<LI>issue of false color condition...CCDs have very steep cutoff filters...presumes a certain light source...none render yellow, red, orange accurately, can read target then photograph organic dye or pigment .... red spike occurs at 640-680 nanometers...film sees what eye doesn't...roundabout way of saying that color targets are useless
<LI>Franziska Frey stated that "the only way to get around this problem" will be through the development and deployment of multi-channel, multi-spectral cameras, such as the VASARI Imaging system in use at the National Gallery in London[<a href="#fn3">3</a>]
</UL>
<P><LI><B>File size at capture</b> (as baseline for quality)
<UL>
<LI>cases in which imaging guidelines mandate a specific size without paying attention to the size of the original; who else has thought about this?
<UL>
<LI>possible to tailor specifications to output project-by-project
<LI>Boston Photo interviews client: stated use requirements drive the file size
<LI>stock market has established some standard measures for file size, e.g., 18-20MB suitable for print-the 8 x 10 standard (of 300 dpi on output)...agreement that this is not only viable, but AIC reported cases in which 20MB images were found to be cleaner than 80MB counterparts-Don noted that the imaging industry refers to this phenomenon as "empty magnification"
<LI>agreement that "what we need is way to get bureaucrats away from monolithic thinking"
</UL></UL></UL>

<H3>V.  Next steps  </H3>

<P>The meeting produced strong consensus to move forward on several fronts, including communications, testing, and lobbying.

<OL><LI>Set up listserv

<P><LI>Conduct experiment to test the viability of the ISO 16067 sanctioned tools that are currently available (slanted-edge target and Auto SFR software)
<UL>

<LI>purposes: to identify differences in performance characteristics among many scanners in different environments in scanning the same source; to specify the peak performance of systems in their current configurations; to see if the tool verifies subjective assessments and vice versa
<LI>methodology: scan target that Don gave you at the meeting; send image(s) to Don; Don analyzes data-in part, to identify clues as to how/why variances occur-then reports results to individuals and to group
<LI>perceived benefits: to generate a thought process; as noted by Bill Comstock, "we're a group that uses similar devices...not other people using same set...would be interesting to see how these devices perform in different environment provide us with clues about what affects system performance..."

<LI>depending upon level of interest....expand the source material to include not only the slanted-edge target, but also a challenging (but typical) pictorial image that can be distributed to all parties: it was suggested that a worthwhile goal would be to define a standard instruction set of images that could be used to enlighten this group and other interested parties, such as standards organizations
</UL>
<P><LI>Facilitate standards development.  Our objectives are to inform standards committees about the volume of work being done in our community, and to prove (perhaps by completing item #2 above) there is a practical benefit to integrating imaging performance metrics into the production workflows established to create quality images of persistent value.  (Our belief is that too much standards work is consumer-based.)  Our working premise is that standards groups first need community acceptance, then the money to develop tools and applications. 

<P>With help from Don Williams, Franziska Frey will draft a letter, which the group will endorse, to Ken Parulski or other standards chairs.  

<P><LI>Develop targets of materials (spectral response, contrast, size) similar to the range of historic materials we are asked to digitize.  (No specific proposals were made, but several participants agreed to begin exchanging information and to collaborate if possible.)

<P><LI>Meet again! Alan Newman invited to host the next meeting at the Art Institute of Chicago: some aspect(s) of managing color likely to be the main topic of discussion and/or training.
</oL>
<H3><a name="app1"></a>Appendix I: List of Participants</h3>

Sally Bjork, Photographer, University of Michigan

<P>Steve Chapman, Preservation Librarian for Digital Initiatives, Harvard University Library

<P>Bill Comstock, Manager, Digital Imaging Group and Photography Studio, Harvard College Library

<P>Franziska Frey, Research Scientist, Image Permanence Institute

<P>Hans Hansen, Chief Technology Officer. Octavo

<P>Dan Johnston, Senior Photographer (and manager), University of California, Berkeley

<P>Erik Landsberg, Senior Fine Art Photographer (and manager), Museum of Modern Art

<P>Lee Mandell, Programmer/Analyst, Harvard University Library

<P>David Mathews, Photographer, Museum of Fine Arts, Boston

<P>Jan Merrill-Oldham, Malloy-Rabinowitz Preservation Librarian, Harvard University Library
.
<P>Phil Michel, Digital Conversion Specialist (and manager), Library of Congress Prints & Photographs

<P>Stephanie Mitchell, Photographer, Harvard College Library

<P>Ron Murray, Scientific Photographer, Librarian, Library of Congress Preservation, Directorate

<P>Alan Newman, Executive Director, Imaging Department, Art Institute of Chicago

<P>Steve Puglia, Preservation and Imaging Specialist, National Archives and Records Administration

<P>David Remington, Photographer, Harvard College Library

<P>Mark Roosa, Director for Preservation, Library of Congress

<P>David Semperger, Project Manager, Boston Photo, Inc. 

<P>Peter Siegel, Manager, Digital Imaging and Photography , Harvard University Art Museums/Harvard Fine Arts Library

<P>Don Williams, Imaging Scientist, Eastman Kodak Company, Imaging Research and Development

<P>John Woolf, Digital Imaging Specialist (and manager), Museum of Fine Arts, Boston

<H3><a name="app2"></a>Appendix II: Institutional Practices</H3>

<P>Photographers from eleven studios-2 commercial, 4 museum, 5 library/archives-generously set aside the time to respond to a questionnaire distributed in advance of the meeting.  The responses to these 22 questions are summarized below.  If there is interest among the wider community (e.g., other practitioners, DLF, NINCH, funding agencies), these responses can be mined further as the basis for a report or article.

<P><I>experience</I>
<UL>
<li>photo-intermediates-began scanning 1982-2000.  Average start date is 1993.
<li>direct digital photography-began 1995-2000.  Average start date is 1997.
<li>began working with digital still images (individually) in 1990-1998.  Average start date is 1993
</UL>
<P><I>source materials</I>
<UL>
<LI>varied...photographic prints are largest category of materials that have been selected for digitization to date; majority of studios digitizing several categories of materials (3D, fine art, photographic and photo-mechanical prints, plates, vintage film, film intermediates, papyri, and even "live" shots of exhibitions and people); two studios, however, have specialized in certain formats: LC Prints & Photographs with film scanning; Octavo with book pages
</UL>
<P><I>equipment</I>
<UL>
<LI>sensors:  eight types in use, but two predominate: trilinear sensor in 100% of studios; one chip color area sensor in 45%; three chip color area and color sequential area cameras less common; least common: monochrome sensors (linear and area), CMOS and PMTs
<LI>lighting: varies across the board: fluorescent is top choice in four studios; tungsten top choice in five; strobe top in two; seven of eleven studios use at least two types of lighting
</UL>
<P><I>system monitoring</I>
<UL>
<LI>10 of 11 studios use targets to monitor systems, including color calibration of monitors; several also use targets to create ICC profiles that are subsequently embedded in images
</UL>
<P><I>variations in quality cited due to...</I>
<UL>
<LI>environmental factors (vibration, lighting)
<LI>operator error
<LI>flawed systems (drifting during warm-up, imprecise/uniform lighting, imprecise focus, soft-proofing inconsistencies: CCD "sees" spectral reflectance of organic dyes and pigments differently than human eye or film or spectrophotometer)
</UL>
<P><I>specifications for archival images</I>
<DIR>
<P><U>format</U><BR>
TIFF used 97.3% of all cases (used exclusively in 8 of 11 studios); other formats: BMP, JPG

<P><U>color space</U><BR>
RGB predominates; exceptions: sRGB used in two studios; Adobe 1998 RGB used for 
high-resolution "production archival" files in several studios

<P><U>compression</U><BR>
no compression used in 9 of 11 studios; one commercial studio occasionally saves archival 
images as "maximum quality" JPEGs (at the request of clients); one library studio uses LZW 
compression approximately 30% of the time

<P><U>file size</U> (color images)

<P>... largest in museum studios/Octavo:

<TABLE>
<TR>
<TD>>100 MB</td>
<TD>100% of Octavo's and Harvard Art Museums' images</TD>
</TR>
<TR>
<TD>51-100 MB</TD>
<TD>100% of MoMA's and AIC's images</TD>
</TR>
</TABLE>

<P>in one of the museums (MFA) and all of the libraries and archives studios-including Boston Photo, which typically serves these clients)-file sizes are more widely distributed, with the majority falling below 50 MB:
<TABLE>
<TR>
<TD>> 100 MB</TD>
<TD> 4%</TD>
</TR>
<TR>
<TD>51-100 MB</td>
<td>38%</TD>
</tr>
<TR>
<TD>21-50 MB</TD>
<TD>25% </TD>
</TR>

<TR>
<TD>< 20 MB</TD>
<TD>33% </TD>
</tr>
</TABLE>
<P><U>file size</U> (grayscale images-8 studios reporting; three digitizing only in color)
<table>
<TR>
<TD>> 100 MB</TD>
<TD>1%</TD>
</TR>

<TR>
<TD>51-100 MB</TD>
<TD>2%</TD></TR>
<TR>
<TD>21-50 MB</TD>
<TD>26% </TD>
</TR>

<TR>
<TD>< 20 MB</TD>	
<TD>71%</TD>
</TR>
</TABLE>

<P><U>tone reproduction </U>
<BR>set to aimpoint values: 7 studios; to photographer's choice: 8 studios; both: 4 studios

<P><I>note: this category needs further analysis...need to relate methodology to source materials</I>

<P><U>with targets included in-frame</U> (grayscale and color bars predominate; sometimes rulers)
<BR>10 of 11 studios include targets (LC did not, but 90% of their work was film scanning)
</DIR>

<P><I>specifications for delivery images </I>
<DIR>
<P><U>format</U> 
<TABLE>
<TR><TD>JPEG  </tD> 	
<TD>52% of total (used by 10 of the 10 studios that create deliverables) </tD>
</TR>
<TR><TD>TIFF	</tD>
<TD>28%</tD>
</TR>

<TR>
<TD>SID	</tD>
<TD>10% of total (but used for 99% of images created at the Univ. of Michigan) </tD>
</TR>

<TR>
<TD>PCD	 </tD> 
<TD>6% of total (but used for 60% of imaged created by Boston Photo) </tD>
</TR>

<TR><TD>GIF</tD>	  
<TD>4% of total (but used for 60% of imaged created by NARA) </tD> 
</TR>
</TABLE>
</DIR>
<P><I>for page images of books</I>
<DIR>
<TABLE>
<TR>
<TD>PDF</TD>
<TD>used for 100% of Octavo's products: interface design has been very difficult; Octavo has been evaluating other options; one photographer encouraged them, and the rest of the group, to look at JPEG2000 as a possible solution for many delivery applications</TD>
</tR>
</TABLE>

<P><U>with embedded ICC profile</U>
<BR>always:  30%, never:   30%, sometimes:  40%

<P><U>with targets included in-frame </U>
<BR>yes: 7 studios, no: 4

</DIR>
<P><I>Responses to question, "What is your definition of an 'archival' digital image?"</I>

<P><B>overarching</b><BR>
It can be used for a defined set of purposes and functions for a defined period of time.  Its attributes can be accessed, managed, and maintained.  It might be permanent, a best-copy surrogate, or version of last resort.  

<P>An image that meets required standards, set by departmental policy or project. 

<P><b>pictorial quality</b><BR>
An  archival digital image is a digital image that is created with sufficient quality so as to act as a replacement if the original is lost or damaged. 

<P>An "archival" digital image is a high-quality, lossless image that maintains the highest possible fidelity to the original.  It retains the best possible color and grayscale information (no clipping).  Its format is a widely accepted non-proprietary format, and it allows metadata to be associated with the file to make images easier to retrieve and understand in the future. 

<P>... I place considerable value on the following principle: Other things being equal, the best master file is the one that represents the appearance of the original most precisely and completely. I sometimes imagine a model capture system which could be teamed with a model output (printing) system, so that the output from the printing system would exactly reproduce the appearance of the original document, all without any intervention of human judgment. 
The image should accurately represent the subject or means should be available (the profile, target, etc.) to apply tonal and color corrections. 

<P>RGB, TIFF file scanned or captured with prepress quality device, stored on optical media (CD-R or DVD-R).  

<P><B>longevity </b>
<BR>It is an image that meets certain criteria of longevity and integrity that we have defined based upon the state of technology and a consensus of opinion in our particular moment in time and history. 

<P>Will open correctly in ten years. 

<P>Uncompressed TIFF with color scales, grayscales burned to CD with backup CD offsite.  In general: an application independent and platform independent file with references that indicate its characteristics, stored on "permanent media" with approximately 8-10 years life expectancy.  Redundant storage with expectation to migrate for both media and storage formats within 5-7 years from archive inception. 

<P><B>openness, capability to generate other files</b>
<BR>Suggest using the term "master image file," "archival image file" is an oxymoron.  Cambridge Dictionary-master: an original copy of something, such as a recording or film, from which copies can be made.  

<P>The parent of all derivatives, stored on the most modern media redundantly as per IT best practices.  80-100MB TIFF or smaller depending upon size of original.  (AN/AIC)

<P>We define "archival" digital images as the raw data of highest resolution, without compression, with ICC color profiling.

<P>The digital masters for our image capture projects are by definition our "archival images." They are scanned according to capture guidelines intended to make them useful for many different purposes and to make them easy to manage; and administrative metadata for each image is maintained in a project database. Ordinarily the digital master is the capture file as scanned, with only cropping and rotation allowed as processing.  However, I find a scan from a slide or negative usually requires additional processing to be useful, such as fine color adjustments, dust spotting, etc. In this case a second digital master is archived. 

<P>A file that can be returned to for future derivative generation with minimal processing.  Stored as either raw scan data or in a sufficiently wide gamut of color space as to assure no clipping.  The image should accurately represent the subject or means should be available (the profile, target, etc.) to apply tonal and color corrections.  File format should be non-proprietary and widely-used accepted industry standard with associated metadata.

<H3>Notes</h3>
<P><a name="fn1">1</a>. In this context, "imaging performance" refers to the actual outputs of a system component (such as a lens) or of the entire imaging system.
<P><a name="fn2">2</a>. The Museum of Fine Arts, for example, selected a 6M pixel area-array camera with a clean signal over available line-array camera backs.  They reasoned that, if needed, an acceptable larger file could be created with interpolation.

<P><a name="fn3">3</a>See, Francisco H. Imai and Roy S. Berns, "High-Resolution Multi-Spectral Image Archives - A
 Hybrid Approach," <I>Proceedings of  the Sixth Color Imaging Conference, Society of Imaging Science & Technology, Scottsdale, AZ, November 1998</I>.  Available on-line: <a href="http://www.cis.rit.edu/people/faculty/berns/PDFs/cic6_Imai.pdf">http://www.cis.rit.edu/people/faculty/berns/PDFs/cic6_Imai.pdf</a>.

Imaging Practitioners Meeting, 30 March 2001
<!-- End text -->


<!-- Comments and credits -->

        <p>

		<img src="../img/spacer.gif" alt="" align="top" width="400" height="10"><br>

        <font size="1">

                <a href="mailto:dlf@clir.org">Please

        send comments or suggestions</a>.<br>

	Last updated: <!--#echo var="LAST_MODIFIED" --><br>

        &copy; 2000 Council on Library and Information Resources

		</font></p>

<!-- Link to CLIR pages -->

        <table border="0" cellpadding="0" cellspacing="0"

        width="140">

            <tr>

                <td><img src="../img/clirlogo.gif" alt="CLIR" width="40"

                height="40"></td>

                <td><a href="http://www.clir.org"><font size="2">CLIR Home Page</font><br></a>

</td>

            </tr>

        </table>

        </td>

    </tr>

</table>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-9461551-1");
pageTracker._setDomainName('.diglib.org');
pageTracker._trackPageview();
} catch(err) {}</script>
</body>

</html>

